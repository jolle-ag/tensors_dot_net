{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 3: Gauge Freedom\n",
    "\n",
    "### As found in [here.](https://www.tensors.net/p-tutorial-3)\n",
    "\n",
    "In this tutorial we will learn about manipulating the gauge freedom in TN, and how this freedom can be exploited in order to achieve an optimal decomposition of a tensor within a network. Topics include:\n",
    "\n",
    "* Tree TN\n",
    "* Gauge freedom in TN\n",
    "* Shifting the center of orthogonality\n",
    "* Tensor decompositions within networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from ncon import ncon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree tensor networks\n",
    "\n",
    "In this tutorial we shall focus on TN that do not possess closed loops, i.e. TN called tree TN. These networks possess many nice properties that networks containing closed loops lack and are thus much easier to manipulate. However, most of the results presented in this tutorial regarding gauge freedom can be generalized to the case of TN containing closed loops, as discussed [here](https://arxiv.org/abs/1801.05390).\n",
    "\n",
    "In the following figure we can see an example of a tree TN. If we select a tensor to act as the center (or root node) then it is always possible to understand the tree TN as being composed of a set of distinct **branches** extending from this chosen tensor. The right side of the following figure depicts the four branches that extend from the order-4 tensor $A$ of the left-side of the figure. Importantly, connections between the different branches are not possible in networks without closed loops (hence why the name tree).\n",
    "\n",
    "<img src=\"img/Fig18.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauge freedom\n",
    "\n",
    "Let $T$ be a TN that, under contraction of all internal indices, evaluates to some tensor $D$. In this tutorial we shall be concerned with the uniqueness of the decomposition: is there a different choice of tensors within the TN that will still evaluate to the same tensor $D$?\n",
    "\n",
    "The answer is yes! As shown in the figure below, on any internal index one can introduce a resolution of the identity. However, absorbing one of these matrices into each adjoining tensor does change their content (while leaving the geometry of the TN unchanged). Thus we conclude that there are infinitely many choices of tensors such that the TN product evaluates to some fixed output tensor. We call this the gauge freedom of the TN.\n",
    "\n",
    "<img src=\"img/Fig19.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "While in some respects the gauge freedom is a nuisance, it can also be exploited so simplify many types of operations on TN. Indeed, most TN algorithms require fixing the gauge in a prescribed manner in order to function correctly. We now discuss several ways to fix the gauge in such a way as to create a *center of orthogonality*, and the utility of doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a center of orthogonality\n",
    "\n",
    "**Def**: Let $T:\\{ A, B, C, ... \\}$ be a tree TN, then a tensor $A$ is a center of orthogonality if, for every branch of the network attached to $A$, the branch forms an isometry between its open indices and the index connected to tensor $A$.\n",
    "\n",
    "This is seen easier with an example:\n",
    "\n",
    "<img src=\"img/Fig3.png\" width='500'/>\n",
    "\n",
    "Here, the tensor $A$ from the network $T$ (left-side image) is a center of orthogonality iff the constraints of the right-side image are satisfied, which demand that each of the branches connected to $A$ forms an isometry.\n",
    "\n",
    "We now discuss two different methods for changing the gauge in network $T$ to make any tensor $A$ into center of orthogonality. Later we will reveal the significance of doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: 'Pulling through'\n",
    "\n",
    "Here we describe a method for setting a tensor $A$ within a network $T$ as a center of orthogonality through iterative use of the QR decomposition. (SVD could also be used but QR is computationally faster)\n",
    "\n",
    "The idea is: we transform every individual tensor within a branch into a (properly oriented) isometry, then the entire branch collectively becomes an isometry and thus satisfies the definition of center of orthogonality. The method can be decomposed into 3 steps:\n",
    "\n",
    "1) Begin by orienting each index with an arrow that points towards the chosen center tensor $A$:\n",
    "\n",
    "<img src=\"img/Fig5.png\" width='200'/>\n",
    "\n",
    "2) Then, starting from a tensor at the tip of a branch, perform a QR decomposition on the tensor (under the partition between incoming and outgoing arrows). Next redefine the tensor in question as the orthogonal 'Q' part of the QR decomposition and absorb the 'R' matrix into the tensor connected to the outgoing arrow:\n",
    "\n",
    "<img src=\"img/Fig6.png\" width='300'/>\n",
    "\n",
    "3) Repeat, working inwards, until all tensors are isometric w.r.t. their incoming and outgoing arrows. Tensor $A$ is now a center of orthogonality.\n",
    "\n",
    "<img src=\"img/Fig7.png\" width='300'/>\n",
    "\n",
    "Final result:\n",
    "\n",
    "<img src=\"img/Fig8.png\" width='600'/>\n",
    "\n",
    "Now we will see and example code that implements this method, and we will check that the initial and final TN still contract to the same tensor. Note that we use a convention such that tensor indices are ordered from left-to-right along the bottom, then left-to-right along the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0230833475924413e-15\n"
     ]
    }
   ],
   "source": [
    "# Example: Creating a center of orthogonality by 'pulling through'\n",
    "d = 3\n",
    "\n",
    "# Define tensors\n",
    "A = np.random.rand(d,d,d,d)\n",
    "# branch 1:\n",
    "B = np.random.rand(d,d,d); D = np.random.rand(d,d,d); E = np.random.rand(d,d,d)\n",
    "# branch 3:\n",
    "F = np.random.rand(d,d,d)\n",
    "# branch 4:\n",
    "C = np.random.rand(d,d,d); G = np.random.rand(d,d,d)\n",
    "\n",
    "# Iterate QR decompositions\n",
    "# branch 1:\n",
    "DQ, DR = LA.qr(D.reshape(d**2,d)); DQ = DQ.reshape(d,d,d)\n",
    "EQ, ER = LA.qr(E.reshape(d**2,d)); EQ = EQ.reshape(d,d,d)\n",
    "Bt = ncon([B,DR,ER],[[1,2,-3],[-1,1],[-2,2]]) # DULR ordering of indices\n",
    "\n",
    "BQ, BR = LA.qr(Bt.reshape(d**2,d)); BQ = BQ.reshape(d,d,d)\n",
    "\n",
    "# branch 2:\n",
    "FQ, FR = LA.qr(F.reshape(d**2,d)); FQ = FQ.reshape(d,d,d)\n",
    "\n",
    "# branch 3:\n",
    "GQ, GR = LA.qr(G.reshape(d**2,d)); GQ = GQ.reshape(d,d,d)\n",
    "Ct = ncon([C,GR],[[1,-2,-3],[-1,1]])\n",
    "CQ, CR = LA.qr(Ct.reshape(d**2,d)); CQ = CQ.reshape(d,d,d)\n",
    "\n",
    "# A'\n",
    "Ap = ncon([A,BR,FR,CR],[[1,-2,2,3],[-1,1],[-3,2],[-4,3]])\n",
    "\n",
    "# T is now formed by {Ap, BQ, CQ, DQ, EQ, FQ, GQ}\n",
    "\n",
    "\n",
    "# Check that both TN evaluate to the same tensor\n",
    "# I've used different index notation than the original source\n",
    "connections = [[1,-5,2,3],[4,5,1],[6,-10,3],[-1,-2,4],[-3,-4,5],[-6,-7,2],[-8,-9,6]]\n",
    "H0 = ncon([A,B,C,D,E,F,G], connections)\n",
    "H1 = ncon([Ap,BQ,CQ,DQ,EQ,FQ,GQ], connections)\n",
    "\n",
    "dH = LA.norm(H0-H1)/LA.norm(H0)\n",
    "\n",
    "print(dH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Direct Orthogonalization\n",
    "\n",
    "Using as example the TN from method 1, we'll describe a method for setting $A$ as a center of orthogonality directly using a single eigen-decomposition for each branch. The steps are:\n",
    "\n",
    "1) Begin by computing the positive-definite density matrix $\\rho$ associated to each index about the chosen center $A$; this is given by contracting the open indices from a branch with the corresponding open indices from the conjugate of the branch as seen below.\n",
    "\n",
    "<img src=\"img/Fig26.png\" width='600'/>\n",
    "\n",
    "2) Then compute the principle square root $X_i$ of each of the density matrices $\\rho_i = X_i^\\dagger X_i$.\n",
    "\n",
    "3) Finally, we make the change of gauge on each of the indices of tensor $A$ using the appropriate $X$ matrix and its corresponding inverse, as depicted in the figure below. Tensor $A$ is now a center of orthogonality\n",
    "\n",
    "<img src=\"img/Fig21.png\" width='600'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.889982462987246e-15\n"
     ]
    }
   ],
   "source": [
    "# Example: Creating a center of orthogonality with 'direct orthogonalization'\n",
    "d = 3\n",
    "\n",
    "# Define tensors\n",
    "A = np.random.rand(d,d,d,d)\n",
    "# branch 1:\n",
    "B = np.random.rand(d,d,d); D = np.random.rand(d,d,d); E = np.random.rand(d,d,d)\n",
    "# branch 3:\n",
    "F = np.random.rand(d,d,d)\n",
    "# branch 4:\n",
    "C = np.random.rand(d,d,d); G = np.random.rand(d,d,d)\n",
    "\n",
    "# Compute density matrices and their principle squared roots\n",
    "rho1 = ncon([B,D,E,B,D,E],[[5,6,-2],[1,2,5],[3,4,6],[7,8,-1],[1,2,7],[3,4,8]])\n",
    "rho2 = ncon([F,F],[[1,2,-2],[1,2,-1]])\n",
    "rho3 = ncon([C,G,C,G],[[3,5,-2],[1,2,3],[4,5,-1],[1,2,4]])\n",
    "\n",
    "d1, u1 = LA.eigh(rho1); sq_d1 = np.sqrt(abs(d1))\n",
    "d2, u2 = LA.eigh(rho2); sq_d2 = np.sqrt(abs(d2))\n",
    "d3, u3 = LA.eigh(rho3); sq_d3 = np.sqrt(abs(d3))\n",
    "\n",
    "X1 = u1 @ np.diag(sq_d1) @ u1.T; X1inv = u1 @ np.diag(1./sq_d1) @ u1.T\n",
    "X2 = u2 @ np.diag(sq_d2) @ u2.T; X2inv = u2 @ np.diag(1./sq_d2) @ u2.T\n",
    "X3 = u3 @ np.diag(sq_d3) @ u3.T; X3inv = u3 @ np.diag(1./sq_d3) @ u3.T\n",
    "\n",
    "# Execute gauge changes (part 3)\n",
    "Ap = ncon([A,X1,X2,X3],[[1,-2,2,3],[-1,1],[-3,2],[-4,3]])\n",
    "Bp = ncon([B,X1inv],[[-1,-2,1],[1,-3]])\n",
    "Fp = ncon([F,X2inv],[[-1,-2,1],[1,-3]])\n",
    "Cp = ncon([C,X3inv],[[-1,-2,1],[1,-3]])\n",
    "\n",
    "# T is now formed by: {Ap, Bp, Cp, D, E, Fp, G}\n",
    "\n",
    "# Now check that both TN evaluate to the same tensor\n",
    "connections = [[3,-5,4,5],[1,2,3],[6,-10,5],[-1,-2,1],[-3,-4,2],[-6,-7,4],[-8,-9,6]]\n",
    "H0 = ncon([A,B,C,D,E,F,G],connections)\n",
    "H1 = ncon([Ap,Bp,Cp,D,E,Fp,G],connections)\n",
    "dH = LA.norm(H0 - H1) / LA.norm(H0)\n",
    "\n",
    "print(dH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "\n",
    "Both methods have their own advantages and the preferred method may depend on the specific application in mind.\n",
    "\n",
    "Nevetheless, in practice the second one is typically computationally cheaper and easier to execute. In addition, this method only requires changing the gauge on the indices connected to the center. On the other hand, the former involves changing the gauge on all indices of the TN. There are some applications where it is desirable to make every tensor into an isometry, so the 'pulling through' will be preferred there. Moreover, 'pulling through' can be advantageous if high precision is desired, as the errors due to floating-point arithmetic are lesser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor decompositions within networks\n",
    "\n",
    "In tutorial 2 we described how the SVD can be applied to optimally decompose a tensor into a product with some restricted rank. Here we take this concept a step further and describe how, by creating a center of orthogonality, a tensor within a network can be optimally decomposed as to minimize the global error from the entire network.\n",
    "\n",
    "Let us consider the TN we've been working with in the last 2 methods. If we replace tensor $A$ with some new tensor $A'$, the TN will evaluate to a different tensor. When the TN has $A$ in it we call this tensor to which the network evaluates $H$, and $H'$ when $A'$ is in the TN.\n",
    "\n",
    "**Theorem**: If tensor $A$ is a center of orthogonality, then the local difference between tensors $||A-A'||$ precisely equals the global difference between the networks $||H-H'||$.\n",
    "\n",
    "**Proof**: Because $A$ is a center of orthogonality, each of the branches must contract to the identity with their hermitian conjugates. Thus, when taking the trace, $\\text{tr}(H^\\dagger H) = \\text{tr}(A^\\dagger A)$, and the same happens for $H'$ and $A'$. Similarly, the branches also cancel in the scalar product of $H$ with $H'$, as we've only replaced $A$ by $A'$, but the branches remained unchanged. By the definition of the Frobenius norm, it then follows that $|| H - H' || = || A - A' ||$. Diagramatically, all I'm saying is:\n",
    "\n",
    "<img src=\"img/Fig25.png\" width='600'/>\n",
    "\n",
    "**Corollary**: If the center of orthogonality tensor $A$ is replaced with a product of tensors as $A' = A_L \\cdot A_R$, then the optimal restricted rank approximation for $A$ is also optimal for minimizing the global difference $||H - H'||$.\n",
    "\n",
    "This corollay turns out to be an exceptionally useful result. An important task in many TN algorithms is to decompose a tensor that resides with a network into a product of tensors in such a way as to minimize the global error. For instance, we may wish to replace $A$ with a minimal rank product $A_L \\cdot A_R$ so that it minimizes $|| H - H' ||$.\n",
    "\n",
    "This could've been a demonically hard problem, but this corollary implies a straight-forward solution. What we have to do is to appropriately fix the gauge degrees of freedom so that $A$ gets transformed into a center of orthogonality, which then implies that the global error becomes equivalent to the local error of the decomposition. We can then use the optimal single tensor decomposition based on the SVD (see tutorial 2) which will achieve the desired goal of minimizing the global error $|| H - H' ||$:\n",
    "\n",
    "<img src=\"img/Fig24.png\" width='600'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlook:\n",
    "\n",
    "In this tutorial we have gained an understanding of why TN possess gauge freedom, as well as how to exploit this freedom to create a center of orthogonality, which then greatly helps us in minimizing the global error. Many important TN methods, such as the DMRG algorithm, rely heavily on these concepts. In tutorial 4 we shall consider some extensions to these ideas, focusing more thoroughly on m,ulti-stage tensor decompositions, as well as how gauge freedom can be fixed to bring a network into canonical form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set 3:\n",
    "\n",
    "WeÂ¡re given the following TN:\n",
    "\n",
    "<img src=\"img/Fig27.png\" width='600'/>\n",
    "\n",
    "We also define tensors $B$ and $C$ to be equal to $A$, $B = A = C$, and assume that all tensor indices are of dimension $d = 12$.\n",
    "\n",
    "**(a)** Contract the TN to form the tensor $H$ explicitly. Evaluate the norm $||H||$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17331274.43296364\n"
     ]
    }
   ],
   "source": [
    "# (a) Define A, B, C, form H and evaluate its norm.\n",
    "d = 12\n",
    "\n",
    "A = np.zeros((d,d,d))\n",
    "\n",
    "for i in range(d):\n",
    "    for j in range(d):\n",
    "        for k in range(d):\n",
    "            A[i,j,k] = np.sqrt(i + 2.*j + 3.*k + 6)\n",
    "            \n",
    "B = A\n",
    "C = A\n",
    "\n",
    "H = ncon([A,B,C],[[-1,-2,1],[1,-3,2],[2,-4,-5]])\n",
    "\n",
    "LH = LA.norm(H)\n",
    "\n",
    "print(LH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Use the truncated SVD to optimally decompose $C$ into a rank $\\chi = 2$ product of tensors, $C \\to C_L \\cdot C_R$, as depicted in the following picture. Contract the new TN to for a single tensor $H_1$. Compute the truncation error $\\varepsilon = || H - H_1 || / ||H||$.\n",
    "\n",
    "<img src=\"img/Fig28.png\" width='500'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.458269336371587e-07\n"
     ]
    }
   ],
   "source": [
    "# (b) truncated SVD\n",
    "um,sm,vhm = LA.svd(C.reshape(d,d**2))\n",
    "chi = 2\n",
    "CL = um[:,:chi] @ np.diag(np.sqrt(sm[:chi]))\n",
    "CR = (np.diag(np.sqrt(sm[:chi])) @ vhm[:chi,:]).reshape(chi,d,d)\n",
    "\n",
    "H1 = ncon([A,B,CL,CR],[[-1,-2,1],[1,-3,2],[2,3],[3,-4,-5]])\n",
    "err1 = LA.norm(H - H1) / LH\n",
    "\n",
    "print(err1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Starting from the original network, transform tensor $C$ into a center of orthogonality using the \"pulling through\" method based on the QR decomposition, obtaining a new network of tensors $\\{ A', B', C' \\}$. Evaluate this network to a single tensor $H'$ and then check that $|| H - H' || = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.426172660496248e-09\n"
     ]
    }
   ],
   "source": [
    "# (c) Transform C into center of orthogonality using the \"pulling through\" method\n",
    "\n",
    "AQ, AR = LA.qr(A.reshape(d**2,d)); AQ = AQ.reshape(d,d,d)\n",
    "\n",
    "Bt = ncon([AR,B],[[-1,1],[1,-2,-3]])\n",
    "\n",
    "BQ, BR = LA.qr(Bt.reshape(d**2,d)); BQ = BQ.reshape(d,d,d)\n",
    "\n",
    "Ct = ncon([BR,C],[[-1,1],[1,-2,-3]])\n",
    "\n",
    "Ht = ncon([AQ,BQ,Ct],[[-1,-2,1],[1,-3,2],[2,-4,-5]])\n",
    "\n",
    "print(LA.norm(H - Ht)) # Should be zero. Idk whether 1e-9 is \"zero enough\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Repeat the truncation step from part (b) on the transformed tensor $C' \\to C'_L \\cdot C'_R$, again keeping rank $\\chi = 2$. Contract the new TN $\\{ A', B', C'_L, C'_R \\}$ to form a single tensor $H'_1$. Compute the truncation error $\\varepsilon' = || H - H'_1 || / ||H||$ and confirm that it is smaller than the result from (b). Why is this the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0618346029953804e-07\n"
     ]
    }
   ],
   "source": [
    "# (d) Repeat (b) on the transformed tensor Ct obtained in (c). \n",
    "um,sm,vhm = LA.svd(Ct.reshape(d,d**2))\n",
    "chi = 2\n",
    "CtL = um[:,:chi] @ np.diag(np.sqrt(sm[:chi]))\n",
    "CtR = (np.diag(np.sqrt(sm[:chi])) @ vhm[:chi,:]).reshape(chi,d,d)\n",
    "\n",
    "Ht1 = ncon([AQ,BQ,CtL,CtR],[[-1,-2,1],[1,-3,2],[2,3],[3,-4,-5]])\n",
    "errt = LA.norm(H - Ht1) / LH\n",
    "\n",
    "print(errt)\n",
    "\n",
    "# Truncation error is smaller since Ct is a center of orthogonality, which guarantees \n",
    "# that global truncation error is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
